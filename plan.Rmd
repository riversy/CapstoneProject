---
title: "Capstone Project Report"
subtitle: "Natural Language Processing and Text Prediction using HC Corpora datasets"
author: "Igor Goltsov <riversy@gmail.com>"
output: html_document
---

In this report I would like to share my experience how did I get and clean HC Corpora dataset. How did I get the goals of my *Capstone Project* and how I would like to solve the problem in the next month.

## About the Capstone Project

Everyone knows **SwiftKey**. There's the company who created the application that predict next word while user types text on mobile keyboard. The aim of the Capstone Project is to reproduce that functionality. I will use free [HC Corpora][1] dataset of human texts to build prediction model. It will predict next word then user types some text. 

## Get data

The necessary data was compiled by Coursera team into [one package][2] that I will download and unzip it into project folder. I will use `ensureCorporaIsDownloaded` function that collected in the `utils.R`. So I will load the file using `source()` command and fire `ensureCorporaIsDownloaded()` function. The full source code of the file might be found [here][3]. 

```{r}
source("utils.R")
ensureCorporaIsDownloaded()
```

We have following structure of data folder that was created in the project.

```
data/
  en_US/
    en_US.blogs.txt
    en_US.news.txt
    en_US.twitter.txt
  ...  
    
```

There are three files in the each of language folders. These files contains scrapped data from Blogs, News and Twitter respectivly. On a brief view every file contains one entity per line. I would like to make some measurmets before clean up to realize something about the texts.  

## Basic evaluation

I would like to get some brief evaluation on the data. I need to get quantity of lines for each of corpus to prepare representative sample for each of corpus. 

Let's start from *news*. I will load it into memory first. And define total quantity of lines in the corpus.

```{r, cache=TRUE, warning=FALSE}

# Load news
news.lines <- scan(
  "data/en_US/en_US.news.txt", 
  what = "character", 
  sep = "\n"
)

news.line.length <- length(news.lines)

# Remove original lines to 
# reduce memory usage
rm(news.lines)

news.line.length
```

I will do the same for both *en_US.blogs.txt* and *en_US.twitter.txt*.  

```{r, cache=TRUE, warning=FALSE}

# Load blogs
blog.lines <- scan(
  "data/en_US/en_US.blogs.txt", 
  what = "character", 
  sep = "\n"
)

blog.line.length <- length(blog.lines)

# Remove original lines to 
# reduce memory usage
rm(blog.lines)

blog.line.length

# Load lines
twitter.lines <- scan(
  "data/en_US/en_US.twitter.txt", 
  what = "character", 
  sep = "\n"
)

twitter.line.length <- length(twitter.lines)

# Remove original lines to 
# reduce memory usage
rm(twitter.lines)

twitter.line.length
```

So we can see that there are huge amount of lines in each of datasets. *`r news.line.length`*, *`r blog.line.length`* and *`r twitter.line.length`* lines respectevly for **news**, **blogs** and **twitter** datasets. We will use this data to prepare random samples on the next step. I will display the table with summarized amount of lines per corpus.  

```{r, results='asis', echo=FALSE}
library(pander)
corpus.names <- c(
  "en_US.news.txt",
  "en_US.blogs.txt",
  "en_US.twitter.txt"
)
corpus.lengths <- c(
  news.line.length,
  blog.line.length,
  twitter.line.length
)
summary.table <- data.frame(corpus.names, corpus.lengths)
colnames(summary.table) <- c(
  "Name of Corpus",
  "Amount of lines"
)

panderOptions('table.alignment.default', 'left')
pander(summary.table)

```

## Clean text data

As we can see. The amount of texts is quite huge. It might be danger to load whole data into the memory, but we can easly use advantages of `tm` package. So we will load and clean files obe by one. And after clean we will save it into *.rds* files. 

I created the function `getCleanCorpus()` that used to clean corpus of text. I will also use it further to test prediction model. It's useful to have one function that reduce both source data and query that should be predicted.   

Some cleanup helpers will be loaded from `cleanup.R` file.

```{r, cache=TRUE, warning=FALSE}
library(openNLP)
library(tm) 
library(qdap)

source("cleanup.R")

getCleanCorpus <- function(lines){
  
  # Split corpus to sentences
  lines <- sent_detect(lines, language = "en", model = NULL)
  
  corpus <- VCorpus(VectorSource(lines))
  
  reduceList <- c(
    removeSymbols,
    convertExclamationsToPoints,
    removeDoublePoints,
    normalizeAnd,
    clearProfanity,
    removeNumbers,
    tolower,
    stripWhitespace
  )
  
  
  corpus <- tm_map(corpus, FUN = tm_reduce, tmFuns = reduceList)

  corpus
}

```

For each of file I'll create *R Data Storage* file that store cleaned corpus. Variables that stored in *rds* may be easly reused further. 


```{r cache=TRUE}

# Check if R Data Storage 
# folder exists
if (!file.exists("rds")){
  dir.create("rds")
}

for (rawCorpusFile in list.files("data/en_US")){
  
  rawCorpusPath <- paste0(c("data/en_US", rawCorpusFile), collapse = "/")
  cleanCorpusFile <- paste0(c(rawCorpusFile, "rds"), collapse = ".")
  cleanCorpusPath <- paste0(c("rds", cleanCorpusFile), collapse = "/")
  
  if (!file.exists(cleanCorpusPath)){
    
    lines <- scan(
      rawCorpusPath, 
      what = "character", 
      sep = "\n", 
      skipNul = TRUE
    )
    
    corpus <- getCleanCorpus(lines)
    
    saveRDS(corpus, cleanCorpusPath)
    
    # Clean memory 
    # before next 
    # step
    rm(lines)
    rm(corpus)
  } 
} 

```

## Exploratory analysis

I would like to make some exploratory data analisys for whole corpus. I will load all datasets into one corpus of documents and will explore 1-grams, 2-grams and 3-grams for this collection of data. 








[1]: http://www.corpora.heliohost.org/ "HC corpora is a collection of corpora for various languages freely available to download."
[2]: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip "Coursera-SwiftKey.zip"
[3]: https://github.com/riversy/CapstoneProject/blob/master/utils.R "utils.R"
[4]: https://github.com/riversy/CapstoneProject/blob/master/evaluation.R "evaluation.R"