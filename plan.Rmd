---
title: "Capstone Project Report"
subtitle: "Natural Language Processing and Text Prediction using HC Corpora datasets"
author: "Igor Goltsov <riversy@gmail.com>"
output: html_document
---

In this report I would like to share my experience how did I get and clean HC Corpora dataset. How did I get the goals of my *Capstone Project* and how I would like to solve the problem in the next month.

## About the Capstone Project

Everyone knows **SwiftKey**. There's the company who created the application that predict next word while user types text on mobile keyboard. The aim of the Capstone Project is to reproduce that functionality. I will use free [HC Corpora][1] dataset of human texts to build prediction model. It will predict next word then user types some text. 

## Get data

The necessary data was compiled by Coursera team into [one package][2] that I will download and unzip it into project folder. I will use `ensureCorporaIsDownloaded` function that collected in the `utils.R`. So I will load the file using `source()` command and fire `ensureCorporaIsDownloaded()` function. The full source code of the file might be found [here][3]. 

```{r}
source("utils.R")
ensureCorporaIsDownloaded()
```

We have following structure of data folder that was created in the project.

```
data/
  en_US/
    en_US.blogs.txt
    en_US.news.txt
    en_US.twitter.txt
  ...  
    
```

There are three files in the each of language folders. These files contains scrapped data from Blogs, News and Twitter respectivly. On a brief view every file contains one entity per line. I would like to make some measurmets before clean up to realize something about the texts.  

## Basic evaluation

I would like to get some brief evaluation on the data. I need to get quantity of lines for each of corpus to prepare representative sample for each of corpus. 

Let's start from *news*. I will load it into memory first. And define total quantity of lines in the corpus.

```{r, cache=TRUE, warning=FALSE}

# Load news
news.lines <- scan(
  "data/en_US/en_US.news.txt", 
  what = "character", 
  sep = "\n"
)

news.line.length <- length(news.lines)

# Remove original lines to 
# reduce memory usage
rm(news.lines)

news.line.length
```

I will do the same for both *en_US.blogs.txt* and *en_US.twitter.txt*.  

```{r, cache=TRUE, warning=FALSE}

# Load blogs
blog.lines <- scan(
  "data/en_US/en_US.blogs.txt", 
  what = "character", 
  sep = "\n"
)

blog.line.length <- length(blog.lines)

# Remove original lines to 
# reduce memory usage
rm(blog.lines)

blog.line.length

# Load lines
twitter.lines <- scan(
  "data/en_US/en_US.twitter.txt", 
  what = "character", 
  sep = "\n"
)

twitter.line.length <- length(twitter.lines)

# Remove original lines to 
# reduce memory usage
rm(twitter.lines)

twitter.line.length
```

So we can see that there are huge amount of lines in each of datasets. *`r news.line.length`*, *`r blog.line.length`* and *`r twitter.line.length`* lines respectevly for **news**, **blogs** and **twitter** datasets. We will use this data to prepare random samples on the next step. I will display the table with summarized amount of lines per corpus.  

```{r, results='asis', echo=FALSE}
library(pander)
corpus.names <- c(
  "en_US.news.txt",
  "en_US.blogs.txt",
  "en_US.twitter.txt"
)
corpus.lengths <- c(
  news.line.length,
  blog.line.length,
  twitter.line.length
)
summary.table <- data.frame(corpus.names, corpus.lengths)
colnames(summary.table) <- c(
  "Name of Corpus",
  "Amount of lines"
)

panderOptions('table.alignment.default', 'left')
pander(summary.table)

```

## Clean text data

As we can see. The amount of texts is quite huge. It might be danger to load whole data into the memory, but we can easly use advantages of `tm` package. It will help us to clean the data without complete loading it into memory. 

I created the function `cleanShortCorpus()` that used to clean short corpus of text without valuable memory consumption. I will use it further to cleanup data piece by piece.  

In this function I will split corpus into sentences. After that I will remove all non character symbols and punctuation. Then I will lowercase that and remove possible profanity words. I will use `utils.R` in that I've stored function that loads profanity dictionary

```{r, cache=TRUE, warning=FALSE}
library(openNLP)
library(tm) 
library(qdap)
source("utils.R")

cleanShortCorpus <- function(corpus){
  # TODO: Clean data here 
  
  # Split corpus to sentences
  corpus <- sent_detect(corpus, language = "en", model = NULL)
  
  # Clean corpus from whole punctuation
  cleanCorpus <- character(0)
  j <- 1
  for (i in 1:length(corpus)){
    
    wordsList <- strsplit(corpus[i], "\\W")
    wordsList <- unlist(wordsList)
    
    # Clean from empty words
    wordsList <- wordsList[which(wordsList != "")]
    
    # Transform words to lower 
    wordsList <- tolower(wordsList)

    # Clean profanity
    wordsList <- wordsList[!(wordsList %in% profanityWords())]
    
    # Check if sentence has 
    # three or greater words
    if (length(wordsList) > 2){
      cleanCorpus[j] <- paste(wordsList, collapse = " ")
      j <- j + 1
    }
  }

  cleanCorpus
}

if (!file.exists("clean/en_US.blogs.txt")){
  for (rawCorpusFile in list.files("data/en_US")){
  
    rawCorpusPath <- paste0(c("data/en_US", rawCorpusFile), collapse = "/")
    cleanCorpusPath <- paste0(c("clean", rawCorpusFile), collapse = "/")
    
    if (!file.exists("clean")){
      dir.create("clean")
    }
    
    # Remove clean file from 
    # prevoius execution
    if (file.exists(cleanCorpusPath)){
      unlink(cleanCorpusPath)
    }
  
    raw <- file(rawCorpusPath, open = "rb") 
    clean <- file(cleanCorpusPath, open = "wt") 
    
    readPerTime = 100
  
    lines <- readLines(
                raw, 
                readPerTime, 
                skipNul = TRUE, 
                warn = FALSE,
                encoding = "UTF-8"
            )
    
    
    while (length(lines) > 0){
      
      cleanLines <- cleanShortCorpus(lines)
      
      writeLines(cleanLines, con = clean)
      
      lines <- readLines(
                  raw, 
                  readPerTime, 
                  skipNul = TRUE, 
                  warn = FALSE,
                  encoding = "UTF-8"
              )
    }
    
    close(clean)
    close(raw)
  } 
}


```



## Create samples for test purposes






## Exploratory analysis







[1]: http://www.corpora.heliohost.org/ "HC corpora is a collection of corpora for various languages freely available to download."
[2]: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip "Coursera-SwiftKey.zip"
[3]: https://github.com/riversy/CapstoneProject/blob/master/utils.R "utils.R"
[4]: https://github.com/riversy/CapstoneProject/blob/master/evaluation.R "evaluation.R"